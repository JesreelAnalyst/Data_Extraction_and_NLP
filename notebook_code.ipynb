{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acfa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import string\n",
    "import re\n",
    "from textstat import syllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba72066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract article title and text from URL\n",
    "def extract_article(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Assuming the article title is contained within an <h1> tag\n",
    "    title = soup.find('h1').get_text().strip()\n",
    "    # Assuming the article text is contained within <p> tags\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = '\\n'.join([p.get_text() for p in paragraphs])\n",
    "    return title, text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file\n",
    "df = pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6325829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the text files\n",
    "output_dir = 'article_texts'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7850c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    try:\n",
    "        title, article_text = extract_article(url)\n",
    "        # Save article title and text in a text file\n",
    "        with open(os.path.join(output_dir, f\"{url_id}.txt\"), 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"{title}\\n\\n{article_text}\")\n",
    "        print(f\"Article {url_id} extracted and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting article {url_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive and negative word lists\n",
    "positive_words = set(nltk.corpus.opinion_lexicon.positive())\n",
    "negative_words = set(nltk.corpus.opinion_lexicon.negative())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92146309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text and tokenize\n",
    "def clean_and_tokenize(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word.strip(string.punctuation) for word in tokens if word not in stop_words and word.isalnum()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate sentiment scores\n",
    "def calculate_sentiment_scores(tokens):\n",
    "    positive_score = sum(1 for word in tokens if word in positive_words)\n",
    "    negative_score = sum(1 for word in tokens if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b93b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate other text statistics\n",
    "def calculate_text_statistics(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_words = clean_and_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "    total_complex_words = [word for word in total_words if syllable_count(word) > 2]\n",
    "    complex_word_count = len(total_complex_words)\n",
    "    total_syllables = sum(syllable_count(word) for word in total_words)\n",
    "    avg_sentence_length = len(total_words) / total_sentences\n",
    "    percentage_complex_words = complex_word_count / len(total_words) if len(total_words) > 0 else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = len(total_words) / total_sentences\n",
    "    personal_pronouns_count = len(re.findall(r'\\b(?:I|we|my|ours|us)\\b', text))\n",
    "    avg_word_length = sum(len(word) for word in total_words) / len(total_words) if len(total_words) > 0 else 0\n",
    "    return (\n",
    "        complex_word_count,\n",
    "        len(total_words),\n",
    "        total_syllables,\n",
    "        personal_pronouns_count,\n",
    "        avg_sentence_length,\n",
    "        percentage_complex_words,\n",
    "        fog_index,\n",
    "        avg_words_per_sentence,\n",
    "        avg_word_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a222c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input file\n",
    "df1 = pd.read_excel('input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store individual DataFrames\n",
    "dfs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store the results\n",
    "output_df = pd.DataFrame(columns=[\n",
    "    'URL_ID',\n",
    "    'URL',\n",
    "    'POSITIVE SCORE',\n",
    "    'NEGATIVE SCORE',\n",
    "    'POLARITY SCORE',\n",
    "    'SUBJECTIVITY SCORE',\n",
    "    'AVG SENTENCE LENGTH',\n",
    "    'PERCENTAGE OF COMPLEX WORDS',\n",
    "    'FOG INDEX',\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "    'COMPLEX WORD COUNT',\n",
    "    'WORD COUNT',\n",
    "    'SYLLABLE PER WORD',\n",
    "    'PERSONAL PRONOUNS',\n",
    "    'AVG WORD LENGTH'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform textual analysis for each article\n",
    "for index, row in df1.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    file_path = os.path.join('article_texts', f\"{url_id}.txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            article_text = file.read()\n",
    "        \n",
    "        # Calculate sentiment scores\n",
    "        tokens = clean_and_tokenize(article_text)\n",
    "        positive_score, negative_score, polarity_score, subjectivity_score = calculate_sentiment_scores(tokens)\n",
    "        \n",
    "        # Calculate other text statistics\n",
    "        (\n",
    "            complex_word_count,\n",
    "            word_count,\n",
    "            total_syllables,\n",
    "            personal_pronouns_count,\n",
    "            avg_sentence_length,\n",
    "            percentage_complex_words,\n",
    "            fog_index,\n",
    "            avg_words_per_sentence,\n",
    "            avg_word_length\n",
    "        ) = calculate_text_statistics(article_text)\n",
    "        \n",
    "        # Create DataFrame for current article\n",
    "        article_df = pd.DataFrame({\n",
    "            'URL_ID': [url_id],\n",
    "            'URL': [url],\n",
    "            'POSITIVE SCORE': [positive_score],\n",
    "            'NEGATIVE SCORE': [negative_score],\n",
    "            'POLARITY SCORE': [polarity_score],\n",
    "            'SUBJECTIVITY SCORE': [subjectivity_score],\n",
    "            'AVG SENTENCE LENGTH': [avg_sentence_length],\n",
    "            'PERCENTAGE OF COMPLEX WORDS': [percentage_complex_words],\n",
    "            'FOG INDEX': [fog_index],\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': [avg_words_per_sentence],\n",
    "            'COMPLEX WORD COUNT': [complex_word_count],\n",
    "            'WORD COUNT': [word_count],\n",
    "            'SYLLABLE PER WORD': [total_syllables / word_count if word_count > 0 else 0],\n",
    "            'PERSONAL PRONOUNS': [personal_pronouns_count],\n",
    "            'AVG WORD LENGTH': [avg_word_length]\n",
    "        }, columns=[\n",
    "            'URL_ID',\n",
    "            'URL',\n",
    "            'POSITIVE SCORE',\n",
    "            'NEGATIVE SCORE',\n",
    "            'POLARITY SCORE',\n",
    "            'SUBJECTIVITY SCORE',\n",
    "            'AVG SENTENCE LENGTH',\n",
    "            'PERCENTAGE OF COMPLEX WORDS',\n",
    "            'FOG INDEX',\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "            'COMPLEX WORD COUNT',\n",
    "            'WORD COUNT',\n",
    "            'SYLLABLE PER WORD',\n",
    "            'PERSONAL PRONOUNS',\n",
    "            'AVG WORD LENGTH'\n",
    "        ])\n",
    "        \n",
    "        dfs.append(article_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all individual DataFrames\n",
    "output_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6814fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output DataFrame to Excel\n",
    "output_df.to_excel('output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74adb39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
